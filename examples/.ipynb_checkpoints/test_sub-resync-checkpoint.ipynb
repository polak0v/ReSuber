{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, stats\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import time\n",
    "import pysubs2\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from spatial_transformer_1d import SpatialTransformer1d\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = '/home/ltetrel/Naruto Shippuuden (2007)/Season 18/Naruto Shippuuden - S18E387.mkv'\n",
    "input_file = '/home/ltetrel/Naruto Shippuuden (2007)/Season 01/Naruto Shippuuden - S01E001.mkv'\n",
    "\n",
    "# should list all *.srt files there, so sub-resync is launched for each srt file\n",
    "input_subfile = '/home/ltetrel/Naruto Shippuuden (2007)/Season 01/Naruto Shippuuden - S01E001.fr.srt'\n",
    "\n",
    "output_subfile = input_subfile[:-4] + \"_opt\" + \".srt\"\n",
    "output_wav = \" \".join(input_file.split(\".\")[:-1]) + \".wav\"\n",
    "vocal_file = output_wav[:-4] + \"/vocals_{}.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the audio stream from movie\n",
    "if not os.path.exists(output_wav):\n",
    "    process = subprocess.Popen([\"ffmpeg\", \"-y\", \"-i\", input_file, output_wav]\n",
    "                               , stdout=subprocess.PIPE\n",
    "                               , stderr=subprocess.PIPE)\n",
    "    print(process.stdout.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get audio duration\n",
    "process = subprocess.Popen([\"ffprobe\"\n",
    "                            , \"-v\"\n",
    "                            , \"error\"\n",
    "                            , \"-show_entries\"\n",
    "                            , \"format=duration\"\n",
    "                            , \"-of\"\n",
    "                            , \"default=noprint_wrappers=1:nokey=1\"\n",
    "                            , output_wav]\n",
    "                           , stdout=subprocess.PIPE\n",
    "                           , stderr=subprocess.PIPE)\n",
    "duration_audio_in_ms = int(float(process.stdout.read().decode(\"utf-8\"))*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the vocals from input .wav with spleeter, and load it\n",
    "\n",
    "# Because spleeter uses a lot of RAM, we cannot extract the vocal from the whole file.\n",
    "# Depending on the RAM space, we loop to get all spleeter vocals\n",
    "max_duration_in_ms = 600000\n",
    "vocal_offset = 0\n",
    "nb_iter = int(duration_audio_in_ms/max_duration_in_ms)\n",
    "vocal_resampled = np.array([], dtype=float)\n",
    "\n",
    "for i in range(nb_iter + 1):\n",
    "    if not os.path.exists(vocal_file.format(i)):\n",
    "        process = subprocess.call([\"spleeter\"\n",
    "                                   , \"separate\"\n",
    "                                   , \"--verbose\"\n",
    "                                   , \"-i\"\n",
    "                                   , output_wav\n",
    "                                   , \"-p\"\n",
    "                                   , \"spleeter:2stems\"\n",
    "                                   , \"--offset\"\n",
    "                                   , str(vocal_offset)\n",
    "                                   , \"-d\"\n",
    "                                   , str(max_duration_in_ms/1000)\n",
    "                                   , \"-f\"\n",
    "                                   , \"{filename}/{instrument}_\" + str(i) +\".wav\"\n",
    "                                   , \"-o\"\n",
    "                                   , os.path.dirname(output_wav)]\n",
    "                                   , stdout=subprocess.PIPE)\n",
    "    vocal_offset = vocal_offset + int(max_duration_in_ms/1000)\n",
    "    \n",
    "    # reading vocal stem\n",
    "    fs, vocal = wavfile.read(vocal_file.format(i))\n",
    "    \n",
    "    # sampling the signal (one sample each ms)\n",
    "    sampling_rate = 1000\n",
    "    n_samples = int(vocal.shape[0]*(sampling_rate/fs))\n",
    "    resample_idx = np.insert((fs/sampling_rate)*np.ones(n_samples - 1), 0, 0.)\n",
    "    resample_idx = np.array(np.around(np.cumsum(resample_idx)),dtype=int)\n",
    "    vocal = vocal[resample_idx, 0]\n",
    "    \n",
    "    vocal_resampled = np.concatenate((vocal_resampled, vocal))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal preprocessing\n",
    "vocal_resampled = vocal_resampled**2\n",
    "\n",
    "# We apply a manual threshold which correspond to an audio signal\n",
    "threshold = np.quantile(vocal_resampled, 1/3)\n",
    "vocal_predict = np.array(vocal_resampled > threshold, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the box regressors (aka, the start and stop times)\n",
    "subs = pysubs2.load(input_subfile, encoding=\"utf-8\")\n",
    "\n",
    "# rescaling sub or audio so they have the same length\n",
    "duration_sub_in_ms = subs[-1].end\n",
    "n_subs = len(subs)\n",
    "if duration_sub_in_ms <= duration_audio_in_ms:\n",
    "    duration_in_ms = duration_audio_in_ms\n",
    "else:\n",
    "    duration_in_ms = duration_sub_in_ms\n",
    "    vocal_predict = np.concatenate((vocal_predict, np.zeros(duration_sub_in_ms - duration_audio_in_ms)))\n",
    "\n",
    "# box signal correspond to the subtitles\n",
    "boxes = np.zeros((duration_in_ms,), dtype=int)\n",
    "starts = np.array([], dtype=int)\n",
    "ends = np.array([], dtype=int)\n",
    "for sub in subs:\n",
    "    start = np.int32(sub.start)\n",
    "    end = np.int32(sub.end)\n",
    "    starts = np.append(starts, start)\n",
    "    ends = np.append(ends, end)\n",
    "    boxes[start:end] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for smoothing value for back-propagation\n",
    "def gaussian_kernel_1d(filter_length):\n",
    "    #99% of the values\n",
    "    sigma = (filter_length/2)/2.33\n",
    "    width = int(filter_length/2)\n",
    "\n",
    "    norm = 1.0 / (np.sqrt(2*np.pi) * sigma)\n",
    "    kernel = [norm * np.exp((-1)*(x**2)/(2 * sigma**2)) for x in range(-width, width)]  \n",
    "\n",
    "    return np.float32(kernel / np.sum(kernel))\n",
    "\n",
    "kernel_size = 500\n",
    "f = tf.reshape(gaussian_kernel_1d(kernel_size), [-1, 1, 1])\n",
    "vocal_predict_filtered = tf.reshape(tf.constant(vocal_predict, dtype=tf.float32), [1, -1, 1])\n",
    "vocal_predict_filtered = tf.reshape(tf.nn.conv1d(vocal_predict_filtered, filters=f, stride=1, padding='SAME'), [-1])\n",
    "\n",
    "# kernel_size = kernel_size*4\n",
    "# f = tf.reshape(gaussian_kernel_1d(kernel_size), [-1, 1, 1])\n",
    "boxes_filtered = tf.reshape(tf.constant(boxes, dtype=tf.float32), [1, -1, 1])\n",
    "boxes_filtered = tf.reshape(tf.nn.conv1d(boxes_filtered, filters=f, stride=1, padding='SAME'), [-1])\n",
    "\n",
    "# clustering the subs into n groups\n",
    "max_duration_allowed = 25000\n",
    "mask_boxes_filtered = np.zeros_like(boxes_filtered)\n",
    "num_sub = len(subs)\n",
    "middle_subs = np.array([], dtype=np.int32)\n",
    "for i in range(num_sub - 1):\n",
    "    dura = np.int32(subs[i+1].start) - np.int32(subs[i].end)\n",
    "    if(dura > max_duration_allowed):\n",
    "        middle_sub = np.int32((np.int32(subs[i].end) + np.int32(subs[i+1].start))/2)\n",
    "        middle_subs = np.append(middle_subs, [middle_sub])\n",
    "mask_boxes_filtered[middle_subs] = 1\n",
    "mask_boxes_filtered = np.cumsum(mask_boxes_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, target):\n",
    "    y = y[1:] - y[:-1]\n",
    "    target = target[1:] - target[:-1]\n",
    "    loss = tf.abs(1. - tfp.stats.correlation(x=y, y=target, event_axis=None))\n",
    "    return loss\n",
    "\n",
    "def regularizer(model):\n",
    "    return tf.reduce_mean((model.B[1:] - model.B[:-1])**2)\n",
    "\n",
    "@tf.function\n",
    "def train(model, x, target, l=0.):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model(x), target)\n",
    "        if l > 0.:\n",
    "            loss_value = loss_value + l*regularizer(model)\n",
    "    return loss_value, tape.gradient(loss_value, model.training_vars)\n",
    "\n",
    "# define inputs\n",
    "x = boxes_filtered\n",
    "target = vocal_predict_filtered #derivative remove one point\n",
    "\n",
    "# learning parameters\n",
    "lr_w = 1e-5\n",
    "lr_b = 1e0\n",
    "lr_B = lr_b\n",
    "n_iters = 1000\n",
    "size_x = tf.shape(x)[-1]\n",
    "l = 0\n",
    "\n",
    "model = SpatialTransformer1d()\n",
    "model_non_rigid = SpatialTransformer1d(rigid=False, mask=mask_boxes_filtered, max_offset_range=500)\n",
    "\n",
    "# opt = tf.keras.optimizers.SGD(learning_rate=1e-6)\n",
    "opt_w = tf.keras.optimizers.Adam(learning_rate=lr_w)\n",
    "opt_b = tf.keras.optimizers.Adam(learning_rate=lr_b)\n",
    "opt_B = tf.keras.optimizers.Adam(learning_rate=lr_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.998421052631579, 263.1578947368416]\n",
      "0.8610201\n",
      "------\n",
      "non-rigid - Step: 0 - loss: 0.9264902472496033 - Params: W [0.9984], b [263.1579], B 4.613520954155348e-10\n",
      "\tGrads: W 77.3369140625, b -4.951514711137861e-05, B -8.855233318172395e-06\n",
      "non-rigid - Step: 50 - loss: 0.9183753728866577 - Params: W [0.9984], b [305.3376], B -9.498015403747559\n",
      "\tGrads: W 10.034515380859375, b -2.7455695089884102e-05, B -1.5719397197244689e-06\n",
      "non-rigid - Step: 100 - loss: 0.9164296388626099 - Params: W [0.9983], b [323.7947], B -21.40962028503418\n",
      "\tGrads: W -8.620330810546875, b -1.4183096936903894e-05, B -3.909269253199454e-06\n",
      "non-rigid - Step: 150 - loss: 0.9151971936225891 - Params: W [0.9983], b [327.8819], B -34.62298583984375\n",
      "\tGrads: W -5.166412353515625, b -1.3542114174924791e-05, B -1.8650978290679632e-06\n",
      "non-rigid - Step: 200 - loss: 0.913986086845398 - Params: W [0.9983], b [329.4911], B -48.22711944580078\n",
      "\tGrads: W 14.686935424804688, b 1.7472892068326473e-05, B -3.2211869438469876e-07\n",
      "non-rigid - Step: 250 - loss: 0.9128353595733643 - Params: W [0.9983], b [325.938], B -65.27218627929688\n",
      "\tGrads: W 11.982666015625, b 1.4338816981762648e-05, B 3.0561750463675708e-06\n",
      "non-rigid - Step: 300 - loss: 0.9116052985191345 - Params: W [0.9983], b [320.5145], B -83.44203186035156\n",
      "\tGrads: W 58.97821807861328, b 6.872230733279139e-05, B 7.757409548503347e-06\n",
      "non-rigid - Step: 350 - loss: 0.9103803634643555 - Params: W [0.9983], b [315.5179], B -100.325927734375\n",
      "\tGrads: W 8.858078002929688, b 7.183814886957407e-06, B 4.545211595541332e-06\n",
      "non-rigid - Step: 400 - loss: 0.9095367193222046 - Params: W [0.9982], b [311.3425], B -114.4927978515625\n",
      "\tGrads: W -26.874435424804688, b -2.4948196369223297e-05, B -1.0131245744560147e-06\n",
      "non-rigid - Step: 450 - loss: 0.9087061882019043 - Params: W [0.9982], b [308.1034], B -126.6274185180664\n",
      "\tGrads: W -22.194854736328125, b -2.2711537894792855e-05, B -2.3232776129589183e-06\n",
      "non-rigid - Step: 500 - loss: 0.9077705144882202 - Params: W [0.9982], b [299.6005], B -142.50038146972656\n",
      "\tGrads: W -26.068161010742188, b -1.8756210920400918e-05, B -8.83815346242045e-07\n",
      "non-rigid - Step: 550 - loss: 0.9068145751953125 - Params: W [0.9983], b [281.1425], B -141.9718475341797\n",
      "\tGrads: W -36.21214294433594, b -1.4086253941059113e-05, B 1.6563894860155415e-06\n",
      "non-rigid - Step: 600 - loss: 0.9058002233505249 - Params: W [0.9983], b [254.4136], B -127.814697265625\n",
      "\tGrads: W -26.587844848632812, b -6.960879545658827e-06, B -5.245774445938878e-07\n",
      "non-rigid - Step: 650 - loss: 0.9041386842727661 - Params: W [0.9983], b [223.1344], B -110.5259780883789\n",
      "\tGrads: W 18.338470458984375, b 3.876666596625e-05, B 3.4095928640454076e-06\n",
      "non-rigid - Step: 700 - loss: 0.9026603102684021 - Params: W [0.9983], b [198.2032], B -91.06857299804688\n",
      "\tGrads: W -5.2442169189453125, b 2.3741013137623668e-06, B -9.071027307072654e-08\n",
      "non-rigid - Step: 750 - loss: 0.9018040299415588 - Params: W [0.9983], b [186.071], B -73.88251495361328\n",
      "\tGrads: W 24.212265014648438, b 2.7932546800002456e-05, B 2.8570830181706697e-06\n",
      "non-rigid - Step: 800 - loss: 0.9013292193412781 - Params: W [0.9983], b [179.6201], B -60.961883544921875\n",
      "\tGrads: W 3.6020050048828125, b 7.023147190921009e-06, B 3.8473503991554026e-06\n",
      "non-rigid - Step: 850 - loss: 0.9009599089622498 - Params: W [0.9983], b [177.124], B -50.95970916748047\n",
      "\tGrads: W 36.54075622558594, b 3.6748373531736434e-05, B 9.43141640163958e-06\n",
      "non-rigid - Step: 900 - loss: 0.9006413817405701 - Params: W [0.9983], b [175.4687], B -41.67718505859375\n",
      "\tGrads: W 15.5736083984375, b 1.7008293070830405e-05, B 4.2079491322510876e-06\n",
      "non-rigid - Step: 950 - loss: 0.9003863334655762 - Params: W [0.9983], b [173.8837], B -33.408172607421875\n",
      "\tGrads: W 42.40690612792969, b 3.9374324842356145e-05, B 9.697502719063777e-06\n",
      "18.45 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "### training\n",
    "### rough exploration\n",
    "\n",
    "# offset and weight\n",
    "init_params = [1., 0.]\n",
    "range_min = [-1e-2, -5000]\n",
    "range_max = [1e-2, 5000]\n",
    "\n",
    "weights = [np.linspace(init_params[0] + range_min[0], init_params[0] + range_max[0], 20)\n",
    "           , np.linspace(init_params[1] + range_min[1], init_params[1] + range_max[1], 20)]\n",
    "\n",
    "cost = np.zeros((len(weights[0]), len(weights[1])), dtype=np.float32)\n",
    "\n",
    "# resync\n",
    "# for offset in [params[0]]:\n",
    "for i in range(len(weights[0])):\n",
    "    for j in range(len(weights[1])):\n",
    "        # update model\n",
    "        model.update_weights([weights[0][i]], [weights[1][j]])\n",
    "        # compute cost (convolution of vocals with subtitles)\n",
    "        cost[i, j] = loss(model(x), target)\n",
    "\n",
    "argm = np.unravel_index(np.argmin(cost), cost.shape)\n",
    "params = [weights[0][argm[0]], weights[1][argm[1]]]\n",
    "print(params)\n",
    "print(np.min(cost))\n",
    "print(\"------\")\n",
    "\n",
    "# model.update_weights(W=[params[0]], b=[params[1]])\n",
    "model_non_rigid.update_weights(W=[params[0]], b=[params[1]])\n",
    "### gradient descent\n",
    "for i in range(n_iters):\n",
    "#     loss_value, grads = train(model, x, target)\n",
    "#     if i % 50 == 0:\n",
    "#         print(\"rigid - Step: {} - loss: {} - Params: W {}, b {} - Grad: w {} b: {}\".format(i, loss_value.numpy(), model.W.numpy(), model.b.numpy(), grads[0], grads[1]))\n",
    "    \n",
    "#     opt_w.apply_gradients(zip(grads[:1], model.training_vars[:1]))\n",
    "#     opt_b.apply_gradients(zip(grads[1:], model.training_vars[1:]))\n",
    "#     losses_rigid = np.append(losses_rigid, loss_value)\n",
    "    \n",
    "    loss_value, grads = train(model_non_rigid, x, target, l)\n",
    "    if i % 50 == 0:\n",
    "        print(\"non-rigid - Step: {} - loss: {} - Params: W {}, b {}, B {}\".format(i, loss_value.numpy(), model_non_rigid.W.numpy(), model_non_rigid.b.numpy(), np.mean(model_non_rigid.B.numpy())))\n",
    "        print(\"\\tGrads: W {}, b {}, B {}\".format(np.mean(grads[0]), np.mean(grads[1]), np.mean(grads[2])))\n",
    "        \n",
    "    opt_w.apply_gradients(zip(grads[:1], model_non_rigid.training_vars[:1]))\n",
    "    opt_b.apply_gradients(zip(grads[1:2], model_non_rigid.training_vars[1:2]))\n",
    "    opt_B.apply_gradients(zip(grads[2:], model_non_rigid.training_vars[2:]))\n",
    "    \n",
    "print(\"%.2f s\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving subtitles for rigid\n",
    "# optimal parameters\n",
    "params = [1/model.training_vars[0].numpy(), (-1)*model.training_vars[1].numpy()]\n",
    "print(params)\n",
    "\n",
    "opt_starts = starts*params[0] + params[1]\n",
    "opt_ends = ends*params[0] + params[1]\n",
    "for sub, i in zip(subs, range(len(subs))):\n",
    "    sub.start =  opt_starts[i]\n",
    "    sub.end = opt_ends[i]\n",
    "subs.save(output_subfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving subtitles for non-rigid\n",
    "# optimal parameters\n",
    "params = [1/model_non_rigid.training_vars[0].numpy(), (-1)*model_non_rigid.training_vars[1].numpy(), (-1)*model_non_rigid.training_vars[2].numpy()]\n",
    "print(params)\n",
    "\n",
    "for sub, i in zip(subs, range(len(subs))):\n",
    "    id_mask = int(sub.start)\n",
    "    sub.start = int(params[0]*sub.start + params[1] + params[2][int(sub.start)])\n",
    "    sub.end = int(params[0]*sub.end + params[1] + params[2][int(sub.end)])\n",
    "subs.save(output_subfile + \".non_rigid.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.0018], dtype=float32), array([-377.351], dtype=float32), array([ -18.9268, -165.5825], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# saving subtitles for masked non-rigid\n",
    "# optimal parameters\n",
    "params = [1/model_non_rigid.training_vars[0].numpy(), (-1)*model_non_rigid.training_vars[1].numpy(), (-1)*model_non_rigid.training_vars[2].numpy()]\n",
    "print(params)\n",
    "\n",
    "for sub, i in zip(subs, range(len(subs))):\n",
    "    id_mask = int(mask_boxes_filtered[int(sub.start)])\n",
    "    sub.start = int(params[0]*sub.start + params[1] + params[2][id_mask])\n",
    "    sub.end = int(params[0]*sub.end + params[1] + params[2][id_mask])\n",
    "subs.save(output_subfile + \".non_rigid.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_opt = pysubs2.load(output_subfile, encoding=\"utf-8\")\n",
    "\n",
    "boxes_opt = np.zeros((duration_in_ms,), dtype=np.int32)\n",
    "starts_opt = np.array([], dtype=np.int32)\n",
    "ends_opt = np.array([], dtype=np.int32)\n",
    "for sub in subs_opt:\n",
    "    start = np.int32(sub.start)\n",
    "    end = np.int32(sub.end)\n",
    "    starts_opt = np.append(starts_opt, start)\n",
    "    ends_opt = np.append(ends_opt, end)\n",
    "    boxes_opt[start:end] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(model_non_rigid(x) - target)\n",
    "plt.title(\"non-rigid\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(model(x) - target)\n",
    "plt.title(\"rigid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "plt.plot(target, \"k\")\n",
    "plt.plot(model_non_rigid(x), \"g--\")\n",
    "plt.plot(model(x), \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "# cost function over params\n",
    "plt.figure()\n",
    "plt.imshow(cost[::-1, :], extent=[init_params[1] + range_min[1]\n",
    "                         , init_params[1] + range_max[1]\n",
    "                         , init_params[0] + range_min[0]\n",
    "                         , init_params[0] + range_max[0]], aspect=\"auto\", cmap=\"gray\")\n",
    "plt.xlabel(\"offset\")\n",
    "plt.ylabel(\"ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "# time = np.arange(data.size)/(1000)\n",
    "fig, ax = plt.subplots()\n",
    "# plt.plot(time, data)\n",
    "# plt.plot(time[:vocals.size], vocals)\n",
    "# plt.plot(time, boxes)\n",
    "# plt.plot(vocal_predict)\n",
    "# plt.plot(boxes, \"r\")\n",
    "# plt.plot(boxes_new, \"g\")\n",
    "plt.plot(vocal_predict_filtered, \"k\")\n",
    "plt.plot(boxes_filtered, \"g--\")\n",
    "plt.plot(mask_boxes_filtered / np.max(mask_boxes_filtered), \"g\")\n",
    "# plt.plot(idx_boxes, \"g\")\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}